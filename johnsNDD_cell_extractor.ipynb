{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prerna Singh (edited by John D'Uva)\n",
    "\n",
    "# Isolate cell regions and non-cell regions (using centers of mass) for 3D sliced Clarity images\n",
    "# Use a brain-wide illumination correction for intensity\n",
    "# Extract 10x the non-cell data than cell data by selecting points within 20 pixels x y and z of cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnduva/opt/anaconda3/envs/neuroData3_7/lib/python3.7/site-packages/python_jsonschema_objects/__init__.py:53: UserWarning: Schema version http://json-schema.org/draft-04/schema not recognized. Some keywords and features may not be supported.\n",
      "  self.schema[\"$schema\"]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import csv\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "from cloudvolume import CloudVolume, view\n",
    "from tifffile import imsave\n",
    "from brainlit.utils.session import NeuroglancerSession\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.empty([33792, 25600, len(os.listdir('pickles'))])\n",
    "test = np.load('pickles/image'+str(0)+\".pkl\", allow_pickle=True)\n",
    "test = np.squeeze(test)\n",
    "test = np.asarray(test)\n",
    "type(test)\n",
    "arr[:,:,0] = test\n",
    "\n",
    "# arr[:,:,0].shape\n",
    "# arr = np.empty([2,2,2])\n",
    "# arr[:,:,0] = [2,3] [4\n",
    "# arr[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33792, 25600, 13312, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir = \"s3://open-neurodata/brainlit/brain1\"\n",
    "dir_segments = \"s3://open-neurodata/brainlit/brain1_segments\"\n",
    "vol = CloudVolume(dir, parallel=True, fill_missing=True)\n",
    "vol.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33792, 25600, 109)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (33792,25600,1,1) into shape (33792,25600)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-433264dde237>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# load pickle data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pickles'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pickles/image'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pickles'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (33792,25600,1,1) into shape (33792,25600)"
     ]
    }
   ],
   "source": [
    "# download a whole image stack into a numpy array from the cloud\n",
    "# vol = CloudVolume('https://dlab-colm.neurodata.io/brian-dev/'+date+'_ENTANGLaGFP-'+section+'/Ch1_470_bias_corrected/')\n",
    "# image = vol[:,:,:]\n",
    "\n",
    "if os.path.isdir('pickles'):\n",
    "    arr = np.empty([33792, 25600, len(os.listdir('pickles'))])\n",
    "    print(arr.shape)\n",
    "    # load pickle data\n",
    "    for i in range(0, len(os.listdir('pickles'))):\n",
    "        \n",
    "        \n",
    "        arr[:,:,i] = np.load('pickles/image'+str(i)+\".pkl\", allow_pickle=True)\n",
    "else:\n",
    "    os.makedirs('pickles')\n",
    "    dir = \"s3://open-neurodata/brainlit/brain1\"\n",
    "    dir_segments = \"s3://open-neurodata/brainlit/brain1_segments\"\n",
    "    vol = CloudVolume(dir, parallel=True, fill_missing=True)\n",
    "\n",
    "    for i in range(13312):\n",
    "        image = vol[:, :, i]\n",
    "        # save output so don't have to download every time\n",
    "        pkl.dump(image, open(\"pickles/image\"+str(i)+\".pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vol.info\n",
    "\n",
    "# dataset_name - The dataset this image came from.\n",
    "# layer - Which layer it came from.\n",
    "# mip - Which mip it came from\n",
    "# layer_type - \"image\" or \"segmentation\"\n",
    "# bounds - The bounding box of the cutout\n",
    "# num_channels - Alias for vol.shape[3]\n",
    "# save_images() - Save Z slice PNGs of the current image to ./saved_images for manual inspection\n",
    "#viewer() - Start a local web server (http://localhost:8080) that can view small volumes interactively. \n",
    "# This was recently changed from view as view is a useful numpy method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"s3://open-neurodata/brainlit/brain1\"\n",
    "dir_segments = \"s3://open-neurodata/brainlit/brain1_segments\"\n",
    "\n",
    "# mydict = {'seg_ids':[], 'imgs':[]}\n",
    "ngl_sess = NeuroglancerSession(url=dir, url_segments=dir_segments, mip=0)\n",
    "ngl_sess.pull_bounds_img(ngl_sess.cv.bounds)\n",
    "\n",
    "\n",
    "# for i in range(1000):\n",
    "#     try:\n",
    "#         # pull_voxel(seg_id, v_id, radius)\n",
    "#         img, bbox, idx = ngl_sess.pull_voxel(i, 0, 15) \n",
    "#         # img: 2*nx+1 X 2*ny+1 X 2*nz+1 volume\n",
    "#         mydict['seg_ids'].append(i)\n",
    "#         mydict['imgs'].append(img)\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save each of 'i' z-planes locally (from the tiff stack)\n",
    "for i in range(0,image.shape[2]):\n",
    "    if i <10:\n",
    "        name = \"brian-dev_\"+date+\"_ENTAGLaGP-\"+section+\"_Ch0_470_z0-2160_y0-2560_z000%i\" %i\n",
    "    elif i <100:\n",
    "        name = \"brian-dev_\"+date+\"_ENTAGLaGP-\"+section+\"_Ch0_470_z0-2160_y0-2560_z00%i\" %i\n",
    "    elif i <1000:\n",
    "        name = \"brian-dev_\"+date+\"_ENTAGLaGP-\"+section+\"_Ch0_470_z0-2160_y0-2560_z0%i\" %i\n",
    "    else:\n",
    "        name = \"brian-dev_\"+date+\"_ENTAGLaGP-\"+section+\"_Ch0_470_z0-2160_y0-2560_z%i\" %i\n",
    "    imsave(section+\"/%s.tiff\" %name, image[:,:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nd.arrayimage\n",
    "np.array(image)\n",
    "# image[:,:,0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory\n",
    "file_path_img = os.path.join(section)\n",
    "list = os.listdir(file_path_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the csv with file labels\n",
    "fileLabelCSV = pd.read_csv('/Users/johnduva/Desktop/2020/csvs/E'+section+'-training.csv')\n",
    "\n",
    "#make ordered lists of each coordinate dimension\n",
    "xfileLabelCSV=(fileLabelCSV['Center of Mass (Geometry): X (px)']).to_list()\n",
    "yfileLabelCSV=(fileLabelCSV['Center of Mass (Geometry): Y (px)']).to_list()\n",
    "zfileLabelCSV=(fileLabelCSV['Center of Mass (Geometry): Z (px)']).to_list()\n",
    "      \n",
    "ROI = np.zeros((len(xfileLabelCSV),12,12,12))\n",
    "\n",
    "#create a list of ranges +-6 from the cell coordinate that are invalid when accumulating non-cell regions\n",
    "x_invalid=[]\n",
    "y_invalid=[]\n",
    "z_invalid=[]\n",
    "for index0 in range(0,len(xfileLabelCSV)):\n",
    "    voxel = np.zeros((12,12,12))\n",
    "    #current coordinates\n",
    "    xcoord = int(xfileLabelCSV[index0])\n",
    "    ycoord = int(yfileLabelCSV[index0])\n",
    "    zcoord = int(zfileLabelCSV[index0])\n",
    "    \n",
    "    #for the ncrs\n",
    "    x_invalid+=[*range(int(xfileLabelCSV[index0])-6,int(xfileLabelCSV[index0])+6)]\n",
    "    y_invalid+=[*range(int(yfileLabelCSV[index0])-6,int(yfileLabelCSV[index0])+6)]\n",
    "    z_invalid+=[*range(int(zfileLabelCSV[index0])-6,int(zfileLabelCSV[index0])+6)]\n",
    "    \n",
    "    \n",
    "    #for every z, append the x&ys\n",
    "    for index, i in enumerate(range(zcoord-6,zcoord+6)):\n",
    "        for file in list:\n",
    "            if str(\"tif\") in str(file):\n",
    "                if len(str(i))==1:\n",
    "                    i=str(\"000\"+str(i))\n",
    "                if len(str(i))==2:\n",
    "                    i=str(\"00\"+str(i))\n",
    "                if len(str(i))==3:\n",
    "                    i=str(\"0\"+str(i))\n",
    "                if str(i) in str(file).split(\"_\")[7]:\n",
    "                    scan1 = np.asarray(Image.open(file_path_img + '/'+file))\n",
    "                    scan = np.zeros(scan1.shape)\n",
    "                    scan = cv2.normalize(scan1,scan, 0, 255, cv2.NORM_MINMAX)\n",
    "                    plane = scan[xcoord-6:xcoord+6,ycoord-6:ycoord+6]\n",
    "                    #plt.imshow(plane)\n",
    "                    #plt.show()\n",
    "        voxel[index]=np.asarray(plane)\n",
    "    ROI[index0]=np.asarray(voxel)\n",
    "\n",
    "print('Total number of CellRegions: ', len(zfileLabelCSV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('ROI'+section+'_bias_corrected.npy', ROI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export cell coordinates as .npy\n",
    "xcoordinate=(fileLabelCSV['Center of Mass (Geometry): X (px)']).to_numpy()\n",
    "ycoordinate=(fileLabelCSV['Center of Mass (Geometry): Y (px)']).to_numpy()\n",
    "zcoordinate=(fileLabelCSV['Center of Mass (Geometry): Z (px)']).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of non-coding regions we want to isolate, lets say 515, same as number of cell regions\n",
    "nNCMB = len(zfileLabelCSV)*10\n",
    "NCR = []\n",
    "\n",
    "#have 11 different coordinates for 1 non-cell region\n",
    "for index0 in range(0,len(xfileLabelCSV)):\n",
    "    #current coordinates\n",
    "    xcoord = int(xfileLabelCSV[index0])\n",
    "    ycoord = int(yfileLabelCSV[index0])\n",
    "    zcoord = int(zfileLabelCSV[index0])\n",
    "    ncrcell=[]\n",
    "    while (len(ncrcell)) < 10:\n",
    "        x_random = random.randint(7,20)\n",
    "        y_random = random.randint(7,20)\n",
    "        z_random = random.randint(7,20)\n",
    "        #proceed =1\n",
    "        #print(\"Random x,y,z for Non CMB Region:\", x_random,y_random,z_random)\n",
    "\n",
    "        #for i in x_invalid:\n",
    "        #    if x_random == i:\n",
    "        #        #print('x_false')\n",
    "        #        proceed = 0\n",
    "        #for i in y_invalid:\n",
    "        #    if y_random == i:\n",
    "        #        #print('y_false')\n",
    "        #        proceed = 0\n",
    "        #for i in z_invalid:\n",
    "        #    if z_random == i:\n",
    "        #        #print('z_false')\n",
    "        #        proceed = 0\n",
    "        if len(ncrcell) %8==0:\n",
    "            #print(\"proceeding\")\n",
    "            ncrcell.append([xcoord+x_random,ycoord+y_random,zcoord+z_random])\n",
    "        elif len(ncrcell) %8==1:\n",
    "            #print(\"proceeding\")\n",
    "            ncrcell.append([xcoord-x_random,ycoord+y_random,zcoord+z_random])\n",
    "        elif len(ncrcell) %8==2:\n",
    "            #print(\"proceeding\")\n",
    "            ncrcell.append([xcoord+x_random,ycoord-y_random,zcoord+z_random])\n",
    "        elif len(ncrcell) %8==3:\n",
    "            #print(\"proceeding\")\n",
    "            ncrcell.append([xcoord+x_random,ycoord+y_random,zcoord-z_random])\n",
    "        elif len(ncrcell) %8==4:\n",
    "            #print(\"proceeding\")\n",
    "            ncrcell.append([xcoord+x_random,ycoord-y_random,zcoord-z_random])\n",
    "        elif len(ncrcell) %8==5:\n",
    "            #print(\"proceeding\")\n",
    "            ncrcell.append([xcoord-x_random,ycoord+y_random,zcoord-z_random])\n",
    "        elif len(ncrcell) %8==6:\n",
    "            #print(\"proceeding\")\n",
    "            ncrcell.append([xcoord-x_random,ycoord-y_random,zcoord+z_random])\n",
    "        else:\n",
    "            ncrcell.append([xcoord-x_random,ycoord-y_random,zcoord-z_random])\n",
    "    for i in ncrcell:\n",
    "        NCR.append(i)\n",
    "    #print(len(NCR))\n",
    "print(\"%s non-CMB regions were isolated.\" %(len(NCR)), \"Adding to the master dictionary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCR_total = np.zeros((len(NCR),12,12,12))\n",
    "\n",
    "for index0 in range(0,len(NCR)):\n",
    "    voxel = np.zeros((12,12,12))\n",
    "    \n",
    "    #current coordinates\n",
    "    xcoord = int(NCR[index0][0])\n",
    "    ycoord = int(NCR[index0][1])\n",
    "    zcoord = int(NCR[index0][2])\n",
    "\n",
    "    #for every z, append the x&ys\n",
    "    for index,i in enumerate(range(zcoord-6,zcoord+6)):\n",
    "        for file in list:\n",
    "            if str(\"tif\") in str(file):\n",
    "                if len(str(i))==1:\n",
    "                    i=str(\"000\"+str(i))\n",
    "                if len(str(i))==2:\n",
    "                    i=str(\"00\"+str(i))\n",
    "                if len(str(i))==3:\n",
    "                    i=str(\"0\"+str(i))\n",
    "                if str(i) in str(file).split(\"_\")[7]:\n",
    "                    scan1 = np.asarray(Image.open(file_path_img + '/'+file))\n",
    "                    scan = np.zeros(scan1.shape)\n",
    "                    scan = cv2.normalize(scan1,scan, 0, 255, cv2.NORM_MINMAX)\n",
    "                    plane = scan[xcoord-6:xcoord+6,ycoord-6:ycoord+6]\n",
    "        voxel[index] = np.asarray(plane)\n",
    "    NCR_total[index0]=np.asarray(voxel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('NCR'+section+'_bias_corrected.npy',np.asarray(NCR_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCR[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(scan[NCR[34][0]-30:NCR[34][0]+30,NCR[34][1]-30:NCR[34][1]+30])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(NCR_total[34][5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
